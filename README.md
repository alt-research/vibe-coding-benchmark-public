<p align="center">
  <h1 align="center">ğŸš€ VibeCodingBench</h1>
  <p align="center">
    <strong>The benchmark that measures what AI coding agents actually do in production</strong>
  </p>
  <p align="center">
    <a href="#why-vibecodingbench">Why</a> â€¢
    <a href="#quick-start">Quick Start</a> â€¢
    <a href="#task-categories">Tasks</a> â€¢
    <a href="#evaluation">Evaluation</a> â€¢
    <a href="#leaderboard">Leaderboard</a> â€¢
    <a href="#contributing">Contributing</a>
  </p>
  <p align="center">
    <img src="https://img.shields.io/badge/tasks-180-blue" alt="Tasks">
    <img src="https://img.shields.io/badge/models-15-green" alt="Models">
    <img src="https://img.shields.io/badge/languages-10-orange" alt="Languages">

    <img src="https://img.shields.io/badge/version-1.0.0-brightgreen" alt="Version">
  </p>
</p>

---

## Why VibeCodingBench?

**Existing benchmarks are disconnected from reality.** See our [full thesis](docs/THESIS.md) for detailed analysis.

| Benchmark | Focus | Real-World Signal | Limitation |
|-----------|-------|-------------------|------------|
| HumanEval | Algorithmic puzzles | âŒ Low | Not production code |
| SWE-bench | Bug fixes in 12 repos | âš ï¸ Medium | [63% suspicious patches](https://runloop.ai/blog/swe-bench-deep-dive-unmasking-the-limitations-of-a-popular-benchmark) |
| SWE-bench Pro | Multi-file tasks | âš ï¸ Medium | [70% â†’ 23% performance drop](https://scale.com/leaderboard/swe_bench_pro_public) |
| **VibeCodingBench** | Full-stack features | âœ… **High** | Production-aligned tasks |

### The Evidence

**Developer Time Distribution** ([Sonar Research](https://www.sonarsource.com/blog/how-much-time-do-developers-spend-actually-writing-code/)):
- Writing new code: 32% | Code maintenance: 19% | Testing: 12%
- Developers code only **52 minutes/day** on average

**The Boilerplate Burden** ([GitHub Octoverse 2025](https://github.blog/news-insights/octoverse/)):
- 2.4M repos use Notebooks (+75% YoY)
- 1.9M repos use Dockerfiles (+120% YoY)
- Developers need help with **repetitive patterns**: auth, CRUD, integrations

**SWE-EVO Exposes the Gap** ([arxiv:2512.18470](https://arxiv.org/abs/2512.18470)):
- Best models: 65% on simple fixes â†’ **only 21% on code evolution**
- "Current AI agents struggle with comprehensive planning and execution"

**Quality Beyond Pass Rate** ([Qodo 2025](https://www.qodo.ai/reports/state-of-ai-code-quality/)):
- "Claude Sonnet 4 averaged **2.11 issues per passing task**"
- Pass rate alone hides production risks

**Developer Frustration** ([Stack Overflow 2025](https://survey.stackoverflow.co/2025/)):
- 66% cite "AI solutions almost right, but not quite" as top frustration
- 45% say "debugging AI code is more time-consuming"

## Quick Start

### From Source

```bash
git clone https://github.com/alt-research/vibe-coding-benchmark-public.git
cd coding-model-benchmark
npm install
npm run build

# List tasks
node packages/cli/dist/index.js list

# Run a task with mock agent
node packages/cli/dist/index.js run saas-core/auth/supabase-oauth --agent mock

# Run with real agent (requires API key)
export ANTHROPIC_API_KEY=your_key
node packages/cli/dist/index.js run saas-core/auth/supabase-oauth --agent claude

# Run full evaluation across agents
node packages/cli/dist/index.js eval --agents claude,glm,minimax

# Watch live execution
node packages/cli/dist/index.js run <task-id> --agent claude --live
```

## Task Categories

| Category | Weight | Tasks | Languages | Examples |
|----------|--------|-------|-----------|----------|
| **SaaS Core** | 25% | 20 | TS, Go, Python, Java, Rust | `supabase-oauth`, `jwt-refresh-tokens`, `rbac-permissions` |
| **Glue Code** | 20% | 20 | Python, Go, TS, Java, Rust | `csv-normalizer`, `kafka-producer`, `cdc-pipeline` |
| **AI Integration** | 20% | 20 | Python, TS, Go | `pdf-qa`, `research-agent`, `semantic-search` |
| **Frontend** | 15% | 20 | React, Vue, Svelte, RN | `landing-page`, `data-grid`, `collaborative-editor` |
| **API Integrations** | 10% | 20 | TS, Go, Python, Java | `checkout-session`, `twilio-sms`, `saml-sso` |
| **Code Evolution** | 10% | 20 | TS, Python, Go, Kotlin | `flask-to-fastapi`, `java-to-kotlin`, `secrets-rotation` |

**Total: 180 tasks** across **10 languages** (TypeScript, Python, Go, Java, Kotlin, Rust, C#, React, Vue, Svelte)

### Language Distribution

Based on [GitHub Octoverse 2025](https://github.blog/news-insights/octoverse/) and [Stack Overflow Developer Survey 2025](https://survey.stackoverflow.co/2025/):

| Language | % of Tasks | Rationale |
|----------|------------|-----------|
| TypeScript/JavaScript | 40% | #1 on GitHub, dominant in web dev |
| Python | 25% | #2 on GitHub, AI/ML leader |
| Go | 15% | Rising for cloud-native, microservices |
| Java/Kotlin | 10% | Enterprise, Android development |
| Rust | 5% | Systems programming, performance-critical |
| C# | 5% | Enterprise, game development |

### Task Structure

Each task is a self-contained directory:

```
tasks/saas-core/auth/supabase-oauth/
â”œâ”€â”€ task.yaml           # Metadata, constraints
â”œâ”€â”€ PROMPT.md           # Instructions for the agent
â”œâ”€â”€ tests/              # Evaluation tests
â”‚   â””â”€â”€ auth.test.ts    # Playwright E2E tests
â”œâ”€â”€ docker-compose.yaml # Services (DB, mock APIs)
â””â”€â”€ golden/             # Reference implementation (optional)
```

**Hot-reload support**: Add new tasks while the benchmark is running!

## Evaluation

### Multi-Dimensional Scoring

We measure what senior engineers care about:

| Dimension | Weight | Method | Why It Matters |
|-----------|--------|--------|----------------|
| **Functional** | 40% | Playwright E2E, Pass@k | Does it work? |
| **Visual** | 20% | Pixel diff vs reference | Does it look right? |
| **Quality** | 20% | ESLint + Semgrep + complexity | Is it maintainable? |
| **Cost** | 10% | Tokens used, context pollution | Is it efficient? |
| **Speed** | 10% | Wall-clock time, step count | Is it fast? |

### Security Gate

Any **Critical/High** vulnerability = **automatic fail**. We use Semgrep with OWASP rules.

### The Scoring Formula

```
Final = (Functional Ã— 0.4) + (Visual Ã— 0.2) + (Quality Ã— 0.2)
        - (Cost Penalty) - (Speed Penalty)

Security Fail â†’ Final = 0
```

## Supported Agents

| Agent | Model | Status | Config | Pricing (Input/Output per MTok) |
|-------|-------|--------|--------|--------------------------------|
| Claude | Haiku 4.5 | âœ… Supported | `ANTHROPIC_API_KEY` | $1.00 / $5.00 |
| Claude | Opus 4.5 | âœ… Supported | `ANTHROPIC_API_KEY` | $5.00 / $25.00 |
| Qwen | Qwen3-Max | âœ… Supported | `QWEN_API_KEY` | $1.20 / $6.00 |
| GLM | GLM-4.7 | âœ… Supported | `GLM_API_KEY` | $0.60 / $2.20 |
| MiniMax | M2.1 | âœ… Supported | `MINIMAX_API_KEY` | $0.30 / $1.20 |
| OpenAI | GPT-5.2 | âœ… Supported | `OPENAI_API_KEY` | $1.75 / $14.00 |
| DeepSeek | Chat-v3 | âœ… Supported | `DEEPSEEK_API_KEY` | $0.40 / $1.60 |
| Gemini | 3-Flash Preview | âœ… Supported | `GOOGLE_API_KEY` | $0.50 / $3.00 |

## Leaderboard

```
ğŸ“ˆ LEADERBOARD (2026-01-27) - 180 tasks evaluated, 15 models

â•”â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Rank â”‚ Model                â”‚ Final â”‚ Pass Rate â”‚ Total Cost â”‚ Total Time â”‚ Avg Time/Taskâ”‚ Total Tokensâ•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ #1   â”‚ Claude Opus 4.5      â”‚ 89.2% â”‚ 100.0%    â”‚ $12.31     â”‚ 2h 12m     â”‚ 44s          â”‚ 648K        â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ #2   â”‚ Claude Haiku 4.5     â”‚ 89.0% â”‚ 99.4%     â”‚ $3.03      â”‚ 1h 5m      â”‚ 22s          â”‚ 798K        â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ #3   â”‚ Grok 4 Fast          â”‚ 88.8% â”‚ 98.9%     â”‚ $0.21      â”‚ 1h 57m     â”‚ 70s          â”‚ 520K        â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ #4   â”‚ OpenAI GPT-5.2       â”‚ 88.8% â”‚ 98.3%     â”‚ $5.01      â”‚ 1h 24m     â”‚ 28s          â”‚ 485K        â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ #5   â”‚ Qwen3 Max            â”‚ 88.6% â”‚ 100.0%    â”‚ $5.42      â”‚ 2h 15m     â”‚ 45s          â”‚ 949K        â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ #6   â”‚ Claude Sonnet 4.5    â”‚ 88.6% â”‚ 98.3%     â”‚ $6.98      â”‚ 2h 6m      â”‚ 42s          â”‚ 612K        â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ #7   â”‚ GLM 4-Plus           â”‚ 88.2% â”‚ 98.9%     â”‚ $0.93      â”‚ 4h 49m     â”‚ 96s          â”‚ 794K        â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ #8   â”‚ DeepSeek v3.2        â”‚ 88.2% â”‚ 98.3%     â”‚ $0.50      â”‚ 4h 29m     â”‚ 90s          â”‚ 543K        â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ #9   â”‚ Grok 4               â”‚ 88.0% â”‚ 97.8%     â”‚ $5.47      â”‚ 2h 5m      â”‚ 75s          â”‚ 480K        â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ #10  â”‚ MiniMax M2.1         â”‚ 87.4% â”‚ 99.4%     â”‚ $2.40      â”‚ 8h 15m     â”‚ 165s         â”‚ 2.78M       â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ #11  â”‚ Grok 4.1 Fast        â”‚ 86.8% â”‚ 97.2%     â”‚ $0.24      â”‚ 2h 27m     â”‚ 89s          â”‚ 580K        â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ #12  â”‚ Gemini 3 Pro Preview â”‚ 85.8% â”‚ 95.0%     â”‚ $10.34     â”‚ 1h 36m     â”‚ 32s          â”‚ 738K        â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ #13  â”‚ GLM-4.7              â”‚ 83.9% â”‚ 85.6%     â”‚ $0.73      â”‚ 2h 50m     â”‚ 57s          â”‚ 623K        â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ #14  â”‚ GLM 4.7 Flash        â”‚ 83.8% â”‚ 92.8%     â”‚ $1.11      â”‚ 2h 15m     â”‚ 45s          â”‚ 650K        â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘ #15  â”‚ Gemini 3 Flash       â”‚ 83.4% â”‚ 92.2%     â”‚ $0.86      â”‚ 1h 23m     â”‚ 28s          â”‚ 384K        â•‘
â•šâ•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Pricing (OpenRouter 2026-01-27)

| Model | Input $/M | Output $/M |
|-------|-----------|------------|
| Claude Opus 4.5 | $5.00 | $25.00 |
| Claude Sonnet 4.5 | $3.00 | $15.00 |
| Claude Haiku 4.5 | $1.00 | $5.00 |
| Qwen3 Max | $1.20 | $6.00 |
| OpenAI GPT-5.2 | $1.75 | $14.00 |
| Grok 4 | $3.00 | $15.00 |
| Grok 4 Fast | $0.20 | $0.50 |
| Grok 4.1 Fast | $0.20 | $0.50 |
| GLM 4-Plus/4.7 | $0.40 | $1.50 |
| GLM 4.7 Flash | $0.07 | $0.40 |
| DeepSeek v3.2 | $0.30 | $1.20 |
| Gemini 3 Flash | $0.50 | $3.00 |
| Gemini 3 Pro | $2.00 | $12.00 |
| MiniMax M2.1 | $0.27 | $1.12 |

### Detailed Metrics

| Model | Functional | Quality | Cost/Task | Tokens/Task |
|-------|------------|---------|-----------|-------------|
| Claude Opus 4.5 | 85.0% | 80.0% | $0.0684 | 3,599 |
| Claude Haiku 4.5 | 84.5% | 79.6% | $0.0168 | 4,435 |
| Grok 4 Fast | 84.1% | 80.0% | $0.0012 | 2,889 |
| Qwen3 Max | 85.0% | 80.0% | $0.0301 | 5,273 |
| OpenAI GPT-5.2 | 83.6% | 79.6% | $0.0278 | 2,694 |
| Claude Sonnet 4.5 | 83.6% | 80.0% | $0.0388 | 3,400 |
| GLM 4-Plus | 84.1% | 80.0% | $0.0052 | 4,412 |
| DeepSeek v3.2 | 83.6% | 80.0% | $0.0028 | 3,015 |
| Grok 4 | 83.6% | 80.0% | $0.0304 | 2,667 |
| MiniMax M2.1 | 84.5% | 80.0% | $0.0133 | 15,436 |
| Grok 4.1 Fast | 82.6% | 78.7% | $0.0013 | 3,222 |
| Gemini 3 Pro Preview | 80.8% | 77.3% | $0.0574 | 4,099 |
| GLM-4.7 | 72.7% | 79.6% | $0.0041 | 3,464 |
| GLM 4.7 Flash | 78.9% | 79.6% | $0.0062 | 3,611 |
| Gemini 3 Flash | 78.4% | 75.1% | $0.0048 | 2,133 |

**Live Dashboard**: https://vibecoding.llmbench.xyz

## Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for details.

### Adding a New Task

1. **Create task directory**:
   ```bash
   mkdir -p tasks/<category>/<subcategory>/<task-name>
   ```

2. **Add task.yaml**:
   ```yaml
   name: My New Task
   category: saas-core
   difficulty: medium
   stack: nextjs-supabase
   tags: [typescript, auth]
   ```

3. **Write PROMPT.md** with clear requirements

4. **Add tests** (Playwright for web, pytest for Python)

5. **Submit PR** using the template

## Architecture

```
vibecodingbench/
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ cli/              # CLI tool
â”‚   â”œâ”€â”€ evaluator/        # Scoring engine
â”‚   â””â”€â”€ leaderboard/      # Web dashboard
â”œâ”€â”€ tasks/                # 120 benchmark tasks
â”‚   â”œâ”€â”€ saas-core/        # 20 tasks
â”‚   â”œâ”€â”€ glue-code/        # 20 tasks
â”‚   â”œâ”€â”€ ai-integration/   # 20 tasks
â”‚   â”œâ”€â”€ frontend/         # 20 tasks
â”‚   â”œâ”€â”€ api-integrations/ # 20 tasks
â”‚   â””â”€â”€ code-evolution/   # 20 tasks
â”œâ”€â”€ templates/            # Starter codebases
â”‚   â”œâ”€â”€ nextjs-supabase/
â”‚   â”œâ”€â”€ fastapi-postgres/
â”‚   â”œâ”€â”€ go-fiber/
â”‚   â””â”€â”€ rust-axum/
â””â”€â”€ docker/               # Base images
```

## Deployment

### Self-Hosted (Docker)

```bash
# Build and run production stack
./scripts/deploy.sh docker

# Or in background
./scripts/deploy.sh docker --detach

# Services available at:
# - Dashboard: http://localhost:3000
# - API: http://localhost:3001
```

### Fly.io

```bash
cd packages/leaderboard
fly launch --config fly.toml
fly deploy
```

## Environment Setup

```bash
# Required
export ANTHROPIC_API_KEY=...            # Claude (Anthropic)
export OPENAI_API_KEY=...               # OpenAI
export GOOGLE_API_KEY=...               # Gemini (Google AI)

# Optional
export GLM_API_KEY=...                  # GLM (Zhipu AI)
export MINIMAX_API_KEY=...              # MiniMax
export QWEN_API_KEY=...                 # Qwen (Alibaba DashScope)
export DEEPSEEK_API_KEY=...             # DeepSeek
```

## Citation

If you use VibeCodingBench in your research, please cite:

```bibtex
@software{vibecodingbench2025,
  title = {VibeCodingBench: A Benchmark for AI Coding Agents on Real-World Developer Tasks},
  year = {2025},
  url = {https://github.com/alt-research/vibe-coding-benchmark-public}
}
```


---

<p align="center">
  <sub>Built with â¤ï¸ by the open-source community</sub>
</p>
